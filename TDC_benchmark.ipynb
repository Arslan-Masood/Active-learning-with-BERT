{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "Found local copy...\n",
      "generating training, validation splits...\n",
      "100%|██████████| 379/379 [00:00<00:00, 2094.73it/s]\n",
      "generating training, validation splits...\n",
      "100%|██████████| 379/379 [00:00<00:00, 2110.23it/s]\n",
      "generating training, validation splits...\n",
      "100%|██████████| 379/379 [00:00<00:00, 2074.94it/s]\n",
      "generating training, validation splits...\n",
      "100%|██████████| 379/379 [00:00<00:00, 2071.13it/s]\n",
      "generating training, validation splits...\n",
      "100%|██████████| 379/379 [00:00<00:00, 2068.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "group = admet_group(path = 'data/')\n",
    "data_dir = \"/projects/home/mmasood1/arslan_data_repository/DILI\"\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    benchmark = group.get('DILI') \n",
    "    # all benchmark names in a benchmark group are stored in group.dataset_names\n",
    "    name = benchmark['name']\n",
    "    train_val, test = benchmark['train_val'], benchmark['test']\n",
    "    train, valid = group.get_train_valid_split(benchmark = name, split_type = 'default', seed = seed)\n",
    "    seed_dir = data_dir + f\"/seed_{seed}/\"\n",
    "    os.makedirs(seed_dir, exist_ok = True)\n",
    "    train.to_csv(seed_dir + f\"train_seed_{seed}.csv\", index = False)\n",
    "    valid.to_csv(seed_dir + f\"valid_seed_{seed}.csv\", index = False)\n",
    "    test.to_csv(seed_dir + f\"test_seed_{seed}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023.03.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "#IPythonConsole.drawOptions.comicMode=True\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.info')\n",
    "import rdkit\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "print(rdkit.__version__)\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(smiles,remover=SaltRemover()):\n",
    "    config = {}\n",
    "    config[\"StandardizeSmiles\"] = True\n",
    "    config[\"FragmentParent\"] = False\n",
    "    config[\"SaltRemover\"] = True\n",
    "    config[\"isomericSmiles\"] = False\n",
    "    config[\"kekuleSmiles\"] = True\n",
    "    config[\"canonical\"] = True\n",
    "    # follows the steps in\n",
    "    # https://github.com/rdkit/rdkit/blob/master/Docs/Notebooks/MolStandardize.ipynb\n",
    "    try:\n",
    "        if config[\"StandardizeSmiles\"]:\n",
    "            # removeHs, disconnect metal atoms, normalize the molecule, reionize the molecule\n",
    "\n",
    "                smiles = rdMolStandardize.StandardizeSmiles(smiles)\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        # remove salts\n",
    "        if config[\"SaltRemover\"]:\n",
    "            mol = remover.StripMol(mol, dontRemoveEverything=False) \n",
    "\n",
    "        if config[\"FragmentParent\"]:\n",
    "            mol = rdMolStandardize.FragmentParent(mol) \n",
    "\n",
    "        if config[\"kekuleSmiles\"]:\n",
    "            Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        normalized_smiles = Chem.MolToSmiles(mol, \n",
    "                            isomericSmiles = config[\"isomericSmiles\"],\n",
    "                            kekuleSmiles = config[\"kekuleSmiles\"],\n",
    "                            canonical = config[\"canonical\"],\n",
    "                            allHsExplicit = False)\n",
    "        if normalized_smiles == '':\n",
    "            normalized_smiles = np.nan\n",
    "    except:\n",
    "        normalized_smiles = np.nan\n",
    "    return normalized_smiles\n",
    "\n",
    "def normalize_smiles_parallel(smiles_list):\n",
    "    with Pool() as pool:\n",
    "        results = []\n",
    "        total = len(smiles_list)\n",
    "        with tqdm(total=total, ncols=80, desc=\"Processing\") as pbar:\n",
    "            for normalized_smiles in pool.imap(standardize, smiles_list):\n",
    "                results.append(normalized_smiles)\n",
    "                pbar.update(1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████| 325/325 [00:00<00:00, 6594.02it/s]\n",
      "Processing: 100%|█████████████████████████████| 54/54 [00:00<00:00, 3243.76it/s]\n",
      "Processing: 100%|█████████████████████████████| 96/96 [00:00<00:00, 3918.27it/s]\n",
      "Processing: 100%|███████████████████████████| 331/331 [00:00<00:00, 7341.58it/s]\n",
      "Processing: 100%|█████████████████████████████| 48/48 [00:00<00:00, 2582.27it/s]\n",
      "Processing: 100%|█████████████████████████████| 96/96 [00:00<00:00, 4167.30it/s]\n",
      "Processing: 100%|███████████████████████████| 331/331 [00:00<00:00, 6943.00it/s]\n",
      "Processing: 100%|█████████████████████████████| 48/48 [00:00<00:00, 2555.42it/s]\n",
      "Processing: 100%|█████████████████████████████| 96/96 [00:00<00:00, 3314.84it/s]\n",
      "Processing: 100%|███████████████████████████| 331/331 [00:00<00:00, 6586.56it/s]\n",
      "Processing: 100%|█████████████████████████████| 48/48 [00:00<00:00, 2459.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 100%|█████████████████████████████| 96/96 [00:00<00:00, 4388.69it/s]\n",
      "Processing: 100%|███████████████████████████| 331/331 [00:00<00:00, 6008.51it/s]\n",
      "Processing: 100%|█████████████████████████████| 48/48 [00:00<00:00, 2405.80it/s]\n",
      "Processing: 100%|█████████████████████████████| 96/96 [00:00<00:00, 3649.60it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/projects/home/mmasood1/arslan_data_repository/DILI\"\n",
    "dataset_list = [\"train\",\"valid\",\"test\"]\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    for dataset in dataset_list:\n",
    "        seed_dir = data_dir + f\"/seed_{seed}/\"\n",
    "        normlization_dir = seed_dir + \"normalized_data/\"\n",
    "        os.makedirs(normlization_dir, exist_ok = True)\n",
    "\n",
    "        data = pd.read_csv(seed_dir + f\"{dataset}_seed_{seed}.csv\")\n",
    "        total_mol = data.shape[0]\n",
    "\n",
    "        data.drop(\"Drug_ID\", axis = 1, inplace = True)\n",
    "        data.rename(columns= {'Drug':'SMILES'}, inplace =  True)\n",
    "        normalized_smiles_list = normalize_smiles_parallel(data.SMILES.tolist())\n",
    "        data[\"Normalized_SMILES\"] = normalized_smiles_list\n",
    "        data = data[~data.Normalized_SMILES.isnull()]\n",
    "        data.to_csv(normlization_dir + f\"{dataset}_seed_{seed}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BERT representation\n",
    "### change env to molbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from molbert.models.smiles import SmilesMolbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple, Sequence, Any, Dict, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from molbert.utils.featurizer.molfeaturizer import SmilesIndexFeaturizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MolBertFeaturizer:\n",
    "    \"\"\"\n",
    "    This featurizer takes a molbert model and transforms the input data and\n",
    "    returns the representation in the last layer (pooled output and sequence_output).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        featurizer,\n",
    "        device: str = None,\n",
    "        embedding_type: str = 'pooled',\n",
    "        max_seq_len: Optional[int] = None,\n",
    "        permute: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpoint_path: path or S3 location of trained model checkpoint\n",
    "            device: device for torch\n",
    "            embedding_type: method to reduce MolBERT encoding to an output set of features. Default: 'pooled'\n",
    "                Other options are embeddings summed or concat across layers, and then averaged\n",
    "                Raw sequence and pooled output is also available (set to 'dict')\n",
    "                average-sum-[2|4], average-cat-[2,4], average-[1|2|3|4], average-1-cat-pooled, pooled, dict\n",
    "            max_seq_len: used by the tokenizer, SMILES longer than this will fail to featurize\n",
    "                MolBERT was trained with SuperPositionalEncodings (TransformerXL) to decoupled from the training setup\n",
    "                By default the training config is used (128). If you have long SMILES to featurize, increase this value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device or 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embedding_type = embedding_type\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.permute = permute\n",
    "\n",
    "        # load smiles index featurizer\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "        # load model\n",
    "        self.model = model\n",
    "\n",
    "    def __getstate__(self):\n",
    "        self.__dict__.update({'model': self.model.to('cpu')})\n",
    "        self.__dict__.update({'device': 'cpu'})\n",
    "        return self.__dict__\n",
    "\n",
    "    @property\n",
    "\n",
    "    def transform_single(self, smiles: str) -> Tuple[np.ndarray, bool]:\n",
    "        features, valid = self.transform([smiles])\n",
    "        return features, valid[0]\n",
    "\n",
    "    def transform(self, molecules: Sequence[Any]) -> Tuple[Union[Dict, np.ndarray], np.ndarray]:\n",
    "        input_ids, valid = self.featurizer.transform(molecules)\n",
    "\n",
    "        input_ids = self.trim_batch(input_ids, valid)\n",
    "\n",
    "        token_type_ids = np.zeros_like(input_ids, dtype=np.int64)\n",
    "        attention_mask = np.zeros_like(input_ids, dtype=np.int64)\n",
    "\n",
    "        attention_mask[input_ids != 0] = 1\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long, device=self.device)\n",
    "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long, device=self.device)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.model.bert(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        sequence_output, pooled_output = outputs\n",
    "\n",
    "        # set invalid outputs to 0s\n",
    "        valid_tensor = torch.tensor(\n",
    "            valid, dtype=sequence_output.dtype, device=sequence_output.device, requires_grad=False\n",
    "        )\n",
    "\n",
    "        pooled_output = pooled_output * valid_tensor[:, None]\n",
    "        sequence_out = sequence_output * valid_tensor[:, None, None]\n",
    "\n",
    "        sequence_out = sequence_out.detach().cpu().numpy()\n",
    "        pooled_output = pooled_output.detach().cpu().numpy()\n",
    "        out = pooled_output\n",
    "\n",
    "        return out, valid\n",
    "\n",
    "    @staticmethod\n",
    "    def trim_batch(input_ids, valid):\n",
    "\n",
    "        # trim input horizontally if there is at least 1 valid data point\n",
    "        if any(valid):\n",
    "            _, cols = np.where(input_ids[valid] != 0)\n",
    "        # else trim input down to 1 column (avoids empty batch error)\n",
    "        else:\n",
    "            cols = np.array([0])\n",
    "\n",
    "        max_idx: int = int(cols.max().item() + 1)\n",
    "\n",
    "        input_ids = input_ids[:, :max_idx]\n",
    "\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_dict\n",
    "model_weights_dir = '/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k_ADME/without_physchem_head/'\n",
    "pretrained_model_path = '/projects/home/mmasood1/TG GATE/MolBERT/molbert/molbert_100epochs/molbert_100epochs/checkpoints/last.ckpt'\n",
    "data_dir = '/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/SMILES_len_th_128/'\n",
    "pos_weights = \"/projects/home/mmasood1/arslan_data_repository/invitro/invitro_1m/25_04_2024/pos_weights.csv\"\n",
    "metadata_dir = \"/projects/home/mmasood1/trained_model_predictions/SIDER_PreClinical/BERT_finetune/MF/\"\n",
    "model_dir = os.path.dirname(os.path.dirname(pretrained_model_path))\n",
    "hparams_path = os.path.join(model_dir, 'hparams.yaml')\n",
    "# load config\n",
    "with open(hparams_path) as yaml_file:\n",
    "    config_dict = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict['project_name'] = \"BERT_invitro_ADME_pretraining\"\n",
    "config_dict['model_name'] = \"SMILES_len_th_128_Permute_False_PhySchem_False\"\n",
    "\n",
    "config_dict['model_weights_dir'] = model_weights_dir\n",
    "config_dict['pretrained_model_path'] = pretrained_model_path\n",
    "config_dict[\"metadata_dir\"] = metadata_dir\n",
    "config_dict['pos_weights'] = pos_weights\n",
    "config_dict['data_dir'] = data_dir\n",
    "config_dict['train_file'] = data_dir + \"train_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['valid_file'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "config_dict['test_file'] = data_dir + \"test_set_invitro_1m_300k_ADME_filtered.pkl\"\n",
    "\n",
    "config_dict['mode'] = 'classification'\n",
    "config_dict['alpha'] = 0.0\n",
    "config_dict['beta'] = 0.0\n",
    "config_dict['gamma'] = 0.0\n",
    "\n",
    "config_dict['max_epochs'] = 50\n",
    "config_dict['unfreeze_epoch'] = 210\n",
    "config_dict[\"l2_lambda\"] = 0.0\n",
    "config_dict['embedding_size'] = 50\n",
    "config_dict[\"num_physchem_properties\"] = 200\n",
    "config_dict[\"num_invivo_tasks\"] = 0\n",
    "\n",
    "config_dict['optim'] = 'AdamW'#SGD\n",
    "config_dict['loss_type'] = 'BCE'# Focal_loss\n",
    "\n",
    "config_dict['lr'] = 1e-05\n",
    "config_dict[\"BERT_lr\"] = 3e-5\n",
    "config_dict[\"batch_size\"] = 264\n",
    "config_dict[\"seed\"] = 42\n",
    "\n",
    "\n",
    "\n",
    "config_dict['missing'] = 'nan'\n",
    "config_dict['compute_metric_after_n_epochs'] = 5\n",
    "config_dict['return_trainer'] = True\n",
    "config_dict['EarlyStopping'] = False\n",
    "\n",
    "config_dict[\"accelerator\"] = \"gpu\"\n",
    "config_dict[\"device\"] = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "data = pd.read_pickle(config_dict['train_file'])\n",
    "data.drop(['SMILES'], axis = 1, inplace = True)\n",
    "target_names = data.columns.tolist()\n",
    "\n",
    "config_dict[\"output_size\"] = len(target_names)\n",
    "config_dict[\"num_invitro_tasks\"] = len(target_names)\n",
    "config_dict[\"num_of_tasks\"] = len(target_names)\n",
    "\n",
    "config_dict[\"label_column\"] = target_names\n",
    "config_dict[\"selected_tasks\"] = target_names\n",
    "config_dict['num_mols'] = data.shape[0]\n",
    "config_dict['max_seq_length'] = 128\n",
    "config_dict['bert_output_dim'] = 768\n",
    "config_dict['invitro_head_hidden_layer'] = 2048\n",
    "\n",
    "config_dict[\"permute\"] = False\n",
    "\n",
    "config_dict['pretrained_model'] = True\n",
    "config_dict['freeze_level'] = False\n",
    "config_dict[\"gpu\"] = -1\n",
    "config_dict[\"precision\"] = 32\n",
    "config_dict[\"distributed_backend\"] = \"dp\"\n",
    "config_dict[\"pretrained_crash_model\"] = None #\"/projects/home/mmasood1/Model_weights/invitro/invitro_1million/MolBERT/Retrain_on_top_of_BERT/complete_1m_300k/invitro_with_PhysChem/epoch=2-val_f1_score=0.00.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "model = SmilesMolbertModel(config_dict)\n",
    "checkpoint = torch.load(config_dict[\"pretrained_model_path\"], map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "model.eval()\n",
    "model.freeze()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "featurizer = SmilesIndexFeaturizer.bert_smiles_index_featurizer(126, permute = False)\n",
    "f = MolBertFeaturizer(model = model,\n",
    "                        featurizer= featurizer,\n",
    "                        device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_features(data):\n",
    "    SMILES = data.Normalized_SMILES.tolist()\n",
    "    features_all, masks_all = [],[]\n",
    "    for s in tqdm(SMILES):\n",
    "        features, masks = f.transform([s])\n",
    "        features_all.append(features.squeeze())\n",
    "        masks_all.append(masks)\n",
    "\n",
    "    filtered = [mask[0] for mask in masks_all]\n",
    "    features = pd.DataFrame(features_all)\n",
    "    features = features[filtered]\n",
    "\n",
    "    selected_SMILES = data[filtered].Normalized_SMILES.values\n",
    "    features.insert(0,\"SMILES\", selected_SMILES)\n",
    "    filtered_data = data[data.Normalized_SMILES.isin(selected_SMILES)].reset_index(drop = True)\n",
    "    filtered_data.drop(\"SMILES\", axis = 1, inplace = True)\n",
    "    filtered_data.rename(columns = {\"Normalized_SMILES\":\"SMILES\"}, inplace = True)\n",
    "    return filtered_data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/322 [00:00<01:39,  3.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322/322 [00:51<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:06<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 valid 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:16<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:49<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 train 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 valid 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:16<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 test 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:49<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 train 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 valid 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:16<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 test 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:49<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 train 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 valid 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:16<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 test 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:48<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 train 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 valid 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:16<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 test 2\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/projects/home/mmasood1/arslan_data_repository/DILI\"\n",
    "dataset_list = [\"train\",\"valid\",\"test\"]\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    for dataset in dataset_list:\n",
    "\n",
    "        # relevent dirs\n",
    "        seed_dir = data_dir + f\"/seed_{seed}/\"\n",
    "        normlization_dir = seed_dir + \"normalized_data/\"\n",
    "        filtered_data_dir = normlization_dir + \"filtered_data/\"\n",
    "\n",
    "        os.makedirs(filtered_data_dir, exist_ok = True)\n",
    "\n",
    "        data = pd.read_csv(normlization_dir + f\"{dataset}_seed_{seed}.csv\")\n",
    "        total_mol = data.shape[0]\n",
    "\n",
    "        filtered_data, features = get_BERT_features(data)\n",
    "        filtered_mol = filtered_data.shape[0]\n",
    "        removed_mols = total_mol - filtered_mol\n",
    "        print(seed, dataset, removed_mols)\n",
    "        filtered_data.to_csv(filtered_data_dir + f\"{dataset}_filtered_seed_{seed}.csv\", index = False)\n",
    "        features.to_csv(filtered_data_dir + f\"{dataset}_DILI_BERT_features_seed_{seed}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pos_weights\n",
    "data_dir = \"/projects/home/mmasood1/arslan_data_repository/DILI\"\n",
    "\n",
    "for seed in [1,2,3,4,5]:\n",
    "\n",
    "    # relevent dirs\n",
    "    seed_dir = data_dir + f\"/seed_{seed}/\"\n",
    "    normlization_dir = seed_dir + \"normalized_data/\"\n",
    "    filtered_data_dir = normlization_dir + \"filtered_data/\"\n",
    "\n",
    "    train = pd.read_csv(filtered_data_dir + f\"train_filtered_seed_{seed}.csv\")\n",
    "    valid = pd.read_csv(filtered_data_dir + f\"valid_filtered_seed_{seed}.csv\")\n",
    "    test = pd.read_csv(filtered_data_dir + f\"test_filtered_seed_{seed}.csv\")\n",
    "    all_data = pd.concat([train, valid, test], axis = 0)\n",
    "    pos = (all_data.Y == 1).sum()\n",
    "    neg = (all_data.Y == 0).sum()\n",
    "    class_weight = neg/pos\n",
    "    class_weight = pd.DataFrame({\"Targets\":\"Y\",\n",
    "                \"weights\":[class_weight]})\n",
    "    class_weight.to_csv(filtered_data_dir + f\"class_weight_seed_{seed}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Targets</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.982833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Targets   weights\n",
       "0       Y  0.982833"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Targets</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NR-AR</td>\n",
       "      <td>22.511327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NR-AR-LBD</td>\n",
       "      <td>27.514768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NR-AhR</td>\n",
       "      <td>7.527344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NR-Aromatase</td>\n",
       "      <td>18.403333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NR-ER</td>\n",
       "      <td>6.809584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NR-ER-LBD</td>\n",
       "      <td>18.871429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NR-PPAR-gamma</td>\n",
       "      <td>33.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SR-ARE</td>\n",
       "      <td>5.191083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SR-ATAD5</td>\n",
       "      <td>25.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SR-HSE</td>\n",
       "      <td>16.384409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SR-MMP</td>\n",
       "      <td>5.328976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SR-p53</td>\n",
       "      <td>15.014184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Targets    weights\n",
       "0           NR-AR  22.511327\n",
       "1       NR-AR-LBD  27.514768\n",
       "2          NR-AhR   7.527344\n",
       "3    NR-Aromatase  18.403333\n",
       "4           NR-ER   6.809584\n",
       "5       NR-ER-LBD  18.871429\n",
       "6   NR-PPAR-gamma  33.677419\n",
       "7          SR-ARE   5.191083\n",
       "8        SR-ATAD5  25.787879\n",
       "9          SR-HSE  16.384409\n",
       "10         SR-MMP   5.328976\n",
       "11         SR-p53  15.014184"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"/projects/home/mmasood1/arslan_data_repository/Tox21/pos_weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
