{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4da60f-67bc-4678-9848-d0caaef70669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import gc\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "import hjson  # Use HJSON instead of json\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import deepchem as dc\n",
    "from pytorch_lightning import seed_everything\n",
    "from utils.data_utils import (scafoldsplit_train_test, convert_to_dataframe, \n",
    "                               drop_unwanted_tasks, get_initial_set_with_main_and_aux_samples)\n",
    "\n",
    "from utils.data_utils import convert_dataframe_to_dataloader\n",
    "from utils.model_utils import get_pred_with_uncertainities\n",
    "\n",
    "from utils.utils import wandb_init_model, compute_binary_classification_metrics_MT, active_learning_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619d7b96-238d-46d1-bd98-88a113acbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "    def __init__(self, json_file):\n",
    "        \"\"\"Load configuration from a JSON file.\"\"\"\n",
    "        self.config = self.get_config_from_args(json_file)\n",
    "        self.prepare_directories()\n",
    "\n",
    "    def get_config_from_args(self, json_file):\n",
    "        \"\"\"Load configuration from a JSON file.\"\"\"\n",
    "        with open(json_file, 'r') as f:\n",
    "            config = hjson.load(f)\n",
    "        return config\n",
    "\n",
    "    def prepare_directories(self):\n",
    "        \"\"\"\n",
    "        Build directory paths based on config and create the directories if they don't exist.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration containing metadata_dir, sampling_strategy, and main_task.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Updated config with model_weights_dir added.\n",
    "        \"\"\"\n",
    "        t_names = os.path.join(self.config[\"metadata_dir\"], self.config[\"sampling_strategy\"], self.config[\"main_task\"][0])\n",
    "        self.config[\"query_set_dir\"] = os.path.join(t_names, \"query_set\")\n",
    "        self.config[\"result_dir\"] = os.path.join(t_names, \"Results\")\n",
    "        self.config[\"model_weights_dir\"] = os.path.join(t_names, \"model_weights\")\n",
    "\n",
    "        # select device\n",
    "        if self.config[\"gpu\"] == [0]:\n",
    "            self.config[\"device\"]  = torch.device(\"cuda:0\")\n",
    "        if self.config[\"gpu\"] == [1]:\n",
    "            self.config[\"device\"]  = torch.device(\"cuda:1\")\n",
    "        if self.config[\"gpu\"] == [2]:\n",
    "            self.config[\"device\"]  = torch.device(\"cuda:2\")\n",
    "        if self.config[\"gpu\"] == [3]:\n",
    "            self.config[\"device\"]  = torch.device(\"cuda:3\")\n",
    "            \n",
    "        if self.config[\"gpu\"] == None:\n",
    "            self.config[\"device\"]  = \"cpu\"\n",
    "            \n",
    "        print(self.config[\"device\"])\n",
    "\n",
    "        # Create directories\n",
    "        directories = [self.config[\"query_set_dir\"], self.config[\"result_dir\"], self.config[\"model_weights_dir\"]]\n",
    "        try:\n",
    "            for directory in directories:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "                print(f\"Directory created or already exists: {directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory {directory}: {e}\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Data preparation and split function.\"\"\"\n",
    "        data = pd.read_csv(self.config[\"target_file\"])\n",
    "        self.config[\"selected_tasks\"] = list(self.config[\"main_task\"]) + list(self.config[\"aux_task\"] or [])\n",
    "        self.config[\"num_of_tasks\"] = len(self.config[\"selected_tasks\"])\n",
    "\n",
    "        np.random.seed(self.config[\"seed\"])\n",
    "        train_set, test_set = scafoldsplit_train_test(self.config, all_tasks=self.config[\"use_all_tasks_to_split\"])\n",
    "\n",
    "        train_set = drop_unwanted_tasks(train_set, self.config)\n",
    "        test_set = drop_unwanted_tasks(test_set, self.config)\n",
    "\n",
    "        initial_set, train_set = get_initial_set_with_main_and_aux_samples(train_set, self.config)\n",
    "        random_stratified_splitter = dc.splits.RandomStratifiedSplitter()\n",
    "        pool_set, val_set = random_stratified_splitter.train_test_split(train_set, frac_train=0.85, seed=self.config[\"seed\"])\n",
    "\n",
    "        return (convert_to_dataframe(initial_set, self.config[\"selected_tasks\"]), \n",
    "                convert_to_dataframe(val_set, self.config[\"selected_tasks\"]), \n",
    "                convert_to_dataframe(pool_set, self.config[\"selected_tasks\"]), \n",
    "                convert_to_dataframe(test_set, self.config[\"selected_tasks\"]))\n",
    "\n",
    "    def save_initial_set(self, initial_set):\n",
    "        file_path = os.path.join(self.config[\"query_set_dir\"], \"initial_set.csv\")\n",
    "        initial_set.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdeea6a-48aa-492e-8374-06ab53cb0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Directory created or already exists: /scratch/work/masooda1/trained_model_pred/active_learning/ClinTox/BALD/Y/query_set\n",
      "Directory created or already exists: /scratch/work/masooda1/trained_model_pred/active_learning/ClinTox/BALD/Y/Results\n",
      "Directory created or already exists: /scratch/work/masooda1/trained_model_pred/active_learning/ClinTox/BALD/Y/model_weights\n",
      "train_test_features (1082, 768) (271, 768)\n",
      "train_test_targets (1082, 1) (271, 1)\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigManager(\"/scratch/work/masooda1/active_learning/scripts/config.json\")\n",
    "initial_set, val_set, pool_set, test_set = config_manager.prepare_data()\n",
    "config_manager.save_initial_set(initial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b873933-d5ec-4658-b335-6d1a8bc20128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d41c39-b656-4afe-96c2-af7660df8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearningRunner:\n",
    "    def __init__(self, config_manager, initial_set, pool_set, val_set, test_set):\n",
    "        self.config = config_manager.config\n",
    "        self.initial_set = initial_set\n",
    "        self.pool_set = pool_set\n",
    "        self.val_set = val_set\n",
    "        self.test_set = test_set\n",
    "\n",
    "    def generate_model_name(self):\n",
    "        \"\"\"Generate model name string.\"\"\"\n",
    "        return (rf'itteration_{self.config[\"iteration\"]}_s{self.config[\"seed\"]}_alpha_{self.config[\"alpha\"]}_'\n",
    "                rf'gamma_{self.config[\"gamma\"]}_loss_type_{self.config[\"loss_type\"]}_Î»{self.config[\"optm_l2_lambda\"]}')\n",
    "\n",
    "    def model_exists(self):\n",
    "        \"\"\"Check if model exists by looking for query set file.\"\"\"\n",
    "        return os.path.exists(os.path.join(self.config[\"query_set_dir\"], f'query_set_{self.config[\"model_name\"]}.csv'))\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        \"\"\"Convert dataframes to dataloaders.\"\"\"\n",
    "        train_dl = convert_dataframe_to_dataloader(self.initial_set, self.config, shuffle=True, drop_last=True)\n",
    "        val_dl = convert_dataframe_to_dataloader(self.val_set, self.config, shuffle=False, drop_last=False)\n",
    "        pool_dl = convert_dataframe_to_dataloader(self.pool_set, self.config, shuffle=False, drop_last=False)\n",
    "        test_dl = convert_dataframe_to_dataloader(self.test_set, self.config, shuffle=False, drop_last=False)\n",
    "        return train_dl, val_dl, pool_dl, test_dl\n",
    "\n",
    "    def train_model(self, train_dl, val_dl):\n",
    "        \"\"\"Train the model and initialize wandb.\"\"\"\n",
    "        from utils.models import Vanilla_MLP_classifier\n",
    "        self.config[\"training_steps\"] = len(train_dl)\n",
    "        trained_model, run, trainer = wandb_init_model(\n",
    "            model=Vanilla_MLP_classifier, \n",
    "            train_dataloader=train_dl,\n",
    "            val_dataloader=val_dl,\n",
    "            config=self.config, \n",
    "            model_type='MLP'\n",
    "        )\n",
    "        wandb.finish()\n",
    "        return trained_model\n",
    "\n",
    "    def evaluate_model(self, trained_model, test_dl):\n",
    "        \"\"\"Evaluate the trained model and return predictions.\"\"\"\n",
    "        trained_model = trained_model.eval()  # Set model to evaluation mode\n",
    "        targets, pred_mean, pred_var, all_pred = get_pred_with_uncertainities(\n",
    "            test_dl, trained_model,\n",
    "            n_samples=self.test_set.shape[0],\n",
    "            n_classes=self.config[\"num_of_tasks\"],\n",
    "            cal_uncert=False,\n",
    "            num_forward_passes=1\n",
    "        )\n",
    "        return targets, pred_mean\n",
    "\n",
    "    def save_metrics(self, targets, pred_mean, iteration):\n",
    "        \"\"\"Compute and save metrics.\"\"\"\n",
    "        metrics = compute_binary_classification_metrics_MT(targets, pred_mean, missing='nan')\n",
    "        metrics = metrics.append(metrics.mean(), ignore_index=True)\n",
    "        metrics.insert(0, 'Tasks', self.config[\"selected_tasks\"] + ['mean'])\n",
    "\n",
    "        result_dir = os.path.join(self.config[\"metadata_dir\"], self.config[\"sampling_strategy\"], self.config[\"main_task\"][0], \"Results\")\n",
    "        file_path = os.path.join(result_dir, f'iteration_{iteration}_metrics_{self.config[\"model_name\"]}.csv')\n",
    "        with open(file_path, 'w') as file:\n",
    "            metrics.to_csv(file, index=False)\n",
    "\n",
    "        print(metrics.mean())\n",
    "\n",
    "    def acquire_sample(self, trained_model, pool_dl, iteration):\n",
    "        \"\"\"Run an iteration of active learning and update the training and pool sets.\"\"\"\n",
    "        query_set, updated_training_set, updated_pool_set = active_learning_loop(\n",
    "            trained_model, pool_dl, self.initial_set, self.pool_set, self.config\n",
    "        )\n",
    "\n",
    "        # Save the query set\n",
    "        query_set_dir = os.path.join(self.config[\"metadata_dir\"], self.config[\"sampling_strategy\"], self.config[\"main_task\"][0], \"query_set\")\n",
    "        file_path = os.path.join(query_set_dir, f'query_set_{self.config[\"model_name\"]}.csv')\n",
    "        with open(file_path, 'w') as file:\n",
    "            query_set.to_csv(file, index=False)\n",
    "\n",
    "        # Update the initial and pool sets\n",
    "        self.initial_set = updated_training_set.copy()\n",
    "        self.pool_set = updated_pool_set.copy()\n",
    "\n",
    "    def update_sets(self, query_set):\n",
    "        \"\"\"Update training and pool sets if model exists.\"\"\"\n",
    "        _, updated_training_set, updated_pool_set = active_learning_loop(\n",
    "            trained_model=None, pool_dataloader=None, \n",
    "            initial_set=self.initial_set, pool_set=self.pool_set, \n",
    "            config=self.config, query_set=query_set\n",
    "        )\n",
    "        self.initial_set = updated_training_set.copy()\n",
    "        self.pool_set = updated_pool_set.copy()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run active learning iterations.\"\"\"\n",
    "        for iteration in range(self.config[\"num_iterations\"]):\n",
    "            seed_everything(seed=self.config[\"seed\"])\n",
    "            self.config[\"iteration\"] = iteration\n",
    "            self.config[\"model_name\"] = self.generate_model_name()\n",
    "\n",
    "            if self.model_exists():\n",
    "                query_set = pd.read_csv(os.path.join(self.config[\"query_set_dir\"], f'query_set_{self.config[\"model_name\"]}.csv'))\n",
    "                self.update_sets(query_set)\n",
    "            else:\n",
    "                # Get the data loaders\n",
    "                train_dl, val_dl, pool_dl, test_dl = self.get_dataloaders()\n",
    "\n",
    "                # Train the model\n",
    "                trained_model = self.train_model(train_dl, val_dl)\n",
    "  \n",
    "                # Evaluate the model\n",
    "                targets, pred_mean = self.evaluate_model(trained_model, test_dl)\n",
    "\n",
    "                # Save the metrics\n",
    "                self.save_metrics(targets, pred_mean, iteration)\n",
    "\n",
    "                # Run active learning iteration and update sets\n",
    "                self.acquire_sample(trained_model, pool_dl, iteration)\n",
    "\n",
    "                # Clean up memory\n",
    "                del train_dl, val_dl, test_dl, pool_dl, trained_model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                active_procs = mp.active_children()\n",
    "                # Loop through the list of active child processes and terminate them\n",
    "                for proc in active_procs:\n",
    "                    proc.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfe7248-6983-4e47-a535-f8a643581ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sample taken\n",
      "initial_counts 16 initial_poolset_count 906 query_counts 1 updated_training_counts 17 updated_poolset_counts 905\n",
      "No sample taken\n",
      "initial_counts 17 initial_poolset_count 905 query_counts 1 updated_training_counts 18 updated_poolset_counts 904\n",
      "No sample taken\n",
      "initial_counts 18 initial_poolset_count 904 query_counts 1 updated_training_counts 19 updated_poolset_counts 903\n",
      "No sample taken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_counts 19 initial_poolset_count 903 query_counts 1 updated_training_counts 20 updated_poolset_counts 902\n",
      "No sample taken\n",
      "initial_counts 20 initial_poolset_count 902 query_counts 1 updated_training_counts 21 updated_poolset_counts 901\n",
      "No sample taken\n",
      "initial_counts 21 initial_poolset_count 901 query_counts 1 updated_training_counts 22 updated_poolset_counts 900\n",
      "No sample taken\n",
      "initial_counts 22 initial_poolset_count 900 query_counts 1 updated_training_counts 23 updated_poolset_counts 899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n",
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sample taken\n",
      "initial_counts 23 initial_poolset_count 899 query_counts 1 updated_training_counts 24 updated_poolset_counts 898\n",
      "No sample taken\n",
      "initial_counts 24 initial_poolset_count 898 query_counts 1 updated_training_counts 25 updated_poolset_counts 897\n",
      "No sample taken\n",
      "initial_counts 25 initial_poolset_count 897 query_counts 1 updated_training_counts 26 updated_poolset_counts 896\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "active_learning_runner = ActiveLearningRunner(config_manager, initial_set, pool_set, val_set, test_set)\n",
    "active_learning_runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cc50a-bb10-4a4a-84ae-b23e8fcb7a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab7a7c-7676-4ded-9ad2-6ca3e3196817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033fe07-7ebf-4f33-b026-834d79ff4282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_arslan",
   "language": "python",
   "name": "env_arslan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
