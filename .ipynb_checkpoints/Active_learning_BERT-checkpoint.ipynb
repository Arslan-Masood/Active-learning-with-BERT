{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import gc\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"True\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "import hjson  # Use HJSON instead of json\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import deepchem as dc\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/scratch/work/masooda1/active_learning')\n",
    "from pytorch_lightning import seed_everything\n",
    "from utils.data_utils import (read_train_test_files, scafoldsplit_train_test, convert_to_dataframe, \n",
    "                               drop_unwanted_tasks, get_initial_set_with_main_and_aux_samples)\n",
    "\n",
    "from utils.data_utils import convert_dataframe_to_dataloader\n",
    "from utils.model_utils import get_pred_with_uncertainities\n",
    "from utils.utils import wandb_init_model, compute_binary_classification_metrics_MT, active_learning_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_neg(data):\n",
    "    pos = (data.y == 1).sum()\n",
    "    negative = (data.y == 0).sum()\n",
    "    ratio = pos / negative\n",
    "    return pos, negative, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ARRAY= [\n",
    "    \"hia_hou\",\n",
    "    \"pgp_broccatelli\",\n",
    "    \"bioavailability_ma\",\n",
    "    \"cyp2c19_veith\",\n",
    "    \"cyp2d6_veith\",\n",
    "    \"cyp3a4_veith\",\n",
    "    \"cyp1a2_veith\",\n",
    "    \"cyp2c9_veith\",\n",
    "    \"cyp2c9_substrate_carbonmangels\",\n",
    "    \"cyp2d6_substrate_carbonmangels\",\n",
    "    \"cyp3a4_substrate_carbonmangels\"]\n",
    "\n",
    "#DATASET_ARRAY= [\n",
    "#    \"hia_hou\"]\n",
    "\n",
    "for dataset in DATASET_ARRAY:\n",
    "    config = {\n",
    "        \"project_name\": f\"BALD_{dataset}\",\n",
    "    \n",
    "        \"_comment_input_files\": \"Configuration for input data files\",\n",
    "        \"target_file\": f\"/scratch/work/masooda1/datasets/datasets_for_active_learning/filtered_data/TDC_ADME/{dataset}_filtered.csv\",\n",
    "        \"BERT_features_file\": f\"/scratch/work/masooda1/datasets/datasets_for_active_learning/MolBERT_features/MolBERT_{dataset}.csv\",\n",
    "        \"ECFP_features_file\": f\"/scratch/work/masooda1/datasets/datasets_for_active_learning/MF/MF_r2_1024_{dataset}.csv\",\n",
    "        \"pos_weights\": f\"/scratch/work/masooda1/datasets/datasets_for_active_learning/filtered_data/TDC_ADME/{dataset}_pos_ratio.csv\",\n",
    "        \"class_weights\": \"/path/to/class_weights.json\",\n",
    "    \n",
    "        \"_comment_output_dir\": \"Configuration for output directories\",\n",
    "        \"metadata_dir\": f\"/scratch/cs/pml/AI_drug/trained_model_pred/active_learning/ADME/{dataset}/\",\n",
    "        \"wandb_dir\": f\"/scratch/cs/pml/AI_drug/trained_model_pred/active_learning/ADME/{dataset}/wandb/\",\n",
    "        \"wandb_offline\": False,\n",
    "        \"wandb_mode\": \"online\",\n",
    "    \n",
    "        \"_comment_data_split\": \"Configuration for data splitting\",\n",
    "        \"train_test_split_exists\": False,\n",
    "        \"Compound_col\": \"Drug\",\n",
    "        \"train_frac\": 0.8,\n",
    "        \"use_all_tasks_to_split\": True,\n",
    "        \"splitter\": \"RandomSplitter\",\n",
    "    \n",
    "        \"_comment_tasks\": \"Configuration for task selection\",\n",
    "        \"all_tasks\":\"Y\",\n",
    "        \"main_task\": \"Y\",\n",
    "        \"selected_tasks\": \"Y\",\n",
    "        \"main_task_index\":0, \n",
    "        \"aux_task\": None,  \n",
    "        \"aux_task_index\": None,\n",
    "    \n",
    "        \"_comment_features\": \"Configuration for input features\",\n",
    "        \"features_type\": \"FP\",\n",
    "        \"fp_size\": 1024,\n",
    "    \n",
    "        \"_comment_bnn_architecture\": \"Configuration for BNN architecture\",\n",
    "        \"input_dim\": 768,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"depth\" : 1,\n",
    "        \"dropout_p\": 0.5,\n",
    "        \"BatchNorm1d\": True,\n",
    "        \"use_skip_connection\": True,\n",
    "        \"l2_lambda\": 1e-4,\n",
    "        \"optm_l2_lambda\": 1e-4,\n",
    "    \n",
    "        \"_comment_optimization\": \"Configuration for optimization parameters\",\n",
    "        \"optim\": \"Adam\",\n",
    "        \"lr\": 0.001,\n",
    "        \"lr_schedulers\": \"CosineAnnealingLR\",\n",
    "    \n",
    "        \"_comment_losses\": \"Configuration for weighted losses\",\n",
    "        \"loss_type\" : \"BCE\",\n",
    "        \"missing\" : \"nan\",\n",
    "        \"alpha\": 0.0,\n",
    "        \"beta\": 0.0,\n",
    "        \"gamma\":0.0,\n",
    "    \n",
    "        \"_comment_training\": \"Configuration for training parameters\",\n",
    "        \"epochs\": 1000,\n",
    "        \"min_epochs\": 50,\n",
    "        \"num_workers\": 1, \n",
    "        \"compute_metric_after_n_epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"pretrained_model\": False,\n",
    "        \"return_trainer\": True,\n",
    "        \"_comment_gpu\": \"Use [0] for GPU 0, None for CPU\",\n",
    "        \"gpu\": None,\n",
    "        \"_comment_accelerator\": \"Use gpu or cpu\",\n",
    "        \"accelerator\": \"cpu\",\n",
    "    \n",
    "        \"_comment_early_stopping\": \"Configuration for early stopping\",\n",
    "        \"check_val_every_n_epoch\": 1,\n",
    "        \"EarlyStopping\": True,\n",
    "        \"metric_to_monitor\": \"val_BCE_non_weighted\",\n",
    "        \"metric_direction\": \"min\",\n",
    "        \"_comment_patience\": \"Will stop after compute_metric_after_n_epochs * patience\",\n",
    "        \"patience\": 20,\n",
    "    \n",
    "        \"_comment_active_learning\": \"Configuration for active learning parameters\",\n",
    "        \"main_task_initial_set_samples\": 50,\n",
    "        \"num_forward_passes\": 20,\n",
    "        \"num_iterations\": 1000,\n",
    "        \"sampling_strategy\": \"BALD\",\n",
    "        \"n_query\": 1,\n",
    "        \"dataset\": f\"{dataset}\",\n",
    "        \"seed\": 0\n",
    "    }\n",
    "\n",
    "    t_names = os.path.join(config[\"metadata_dir\"], config[\"sampling_strategy\"], config[\"main_task\"])\n",
    "    config[\"query_set_dir\"] = os.path.join(t_names, \"query_set\")\n",
    "    config[\"result_dir\"] = os.path.join(t_names, \"Results\")\n",
    "    config[\"model_weights_dir\"] = os.path.join(t_names, \"model_weights\")\n",
    "    \n",
    "    config[\"num_of_tasks\"] = len(config[\"selected_tasks\"])\n",
    "    config[\"device\"] = \"cpu\"\n",
    "    \n",
    "    config[\"splitter\"] = \"RandomStratifiedSplitter\"\n",
    "    config[\"seed\"] = 88\n",
    "\n",
    "    # Splitting by using deepchem\n",
    "    train_set, test_set = scafoldsplit_train_test(config, all_tasks = True)\n",
    "    \n",
    "    # Calculate statistics for each dataset separately\n",
    "    train_pos, train_neg, train_ratio = compute_pos_neg(train_set)\n",
    "    test_pos, test_neg, test_ratio = compute_pos_neg(test_set)\n",
    "    \n",
    "    # Calculate total by summing train and test\n",
    "    total_pos = train_pos + test_pos\n",
    "    total_neg = train_neg + test_neg\n",
    "    total_ratio = total_pos / total_neg\n",
    "\n",
    "    initial_set, train_set = get_initial_set_with_main_and_aux_samples(train_set, config)\n",
    "    \n",
    "    random_stratified_splitter = dc.splits.RandomStratifiedSplitter()\n",
    "    pool_set, val_set = random_stratified_splitter.train_test_split(train_set, frac_train=0.80, seed=config[\"seed\"])\n",
    "    \n",
    "    initial_pos, initial_neg, initial_ratio = compute_pos_neg(initial_set)\n",
    "    pool_pos, pool_neg, pool_ratio = compute_pos_neg(pool_set)\n",
    "    val_pos, val_neg, val_ratio = compute_pos_neg(val_set)\n",
    "    test_pos, test_neg, test_ratio = compute_pos_neg(test_set)\n",
    "    \n",
    "    # Print in the requested format\n",
    "    print(f\"total_pos, total_neg, pos_to_neg_ratio\\n{total_pos}, {total_neg}, {total_ratio:.4f}\")\n",
    "    print(f\"initial_pos, initial_neg, pos_to_neg_ratio\\n{initial_pos}, {initial_neg}, {initial_ratio:.4f}\")\n",
    "    print(f\"pool_pos, pool_neg, pos_to_neg_ratio\\n{pool_pos}, {pool_neg}, {pool_ratio:.4f}\")\n",
    "    print(f\"val_pos, val_neg, pos_to_neg_ratio\\n{val_pos}, {val_neg}, {val_ratio:.4f}\")\n",
    "    print(f\"test_pos, test_neg, pos_to_neg_ratio\\n{test_pos}, {test_neg}, {test_ratio:.4f}\")\n",
    "\n",
    "    print(f\"#################{dataset}#############################\")\n",
    "    print(f\"{dataset} & {total_pos} & {total_neg} & {initial_pos} & {initial_neg} & {pool_pos} & {pool_neg} & {val_pos} & {val_neg} & {test_pos} & {test_neg}\")\n",
    "    print(\"##############################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25+298+75+99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test_features (965, 768) (241, 768)\n",
      "train_test_targets (965, 1) (241, 1)\n",
      "##############################################\n",
      "total_pos, total_neg, pos_to_neg_ratio 491, 424, 1.16\n",
      "initial_pos, initial_neg, pos_to_neg_ratio 25, 25, 1.00\n",
      "pool_pos, pool_neg, pos_to_neg_ratio 393, 339, 1.16\n",
      "val_pos, val_neg, pos_to_neg_ratio 98, 85, 1.15\n",
      "test_pos, test_neg, pos_to_neg_ratio 129, 112, 1.15\n",
      "pgp_broccatelli & 491 & 424 & 25 & 25 & 393 & 339 & 98 & 85 & 129 & 112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 30, 12.433333333333334)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who cares about deepchem data_object, trash it\n",
    "selected_tasks = config[\"selected_tasks\"] if isinstance(config[\"selected_tasks\"], list) else [config[\"selected_tasks\"]]\n",
    "\n",
    "initial_set = convert_to_dataframe(initial_set, selected_tasks)\n",
    "val_set = convert_to_dataframe(val_set, selected_tasks)\n",
    "pool_set = convert_to_dataframe(pool_set, selected_tasks)\n",
    "test_set = convert_to_dataframe(test_set, selected_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging (50, 768)\n",
      "After merging (60, 768)\n",
      "After merging (113, 768)\n",
      "After merging (343, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /scratch/work/masooda1/.conda_envs/env_arslan/lib/py ...\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /scratch/cs/pml/AI_drug/trained_model_pred/active_learning/ADME/hia_hou/BALD/Y/model_weights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/scratch/work/masooda1/.conda_envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from /scratch/cs/pml/AI_drug/trained_model_pred/active_learning/ADME/hia_hou/BALD/Y/model_weights/model-iter0-Y-epoch=38-val_BCE_non_weighted=8.1021-v3.ckpt\n",
      "Best validation score: 8.10205364227295\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_acc         0.878427\n",
      "f1_score             0.950980\n",
      "specificity          0.928571\n",
      "sensitivity          0.828283\n",
      "roc_auc              0.912698\n",
      "AUPR                 0.986843\n",
      "average_precision    0.986916\n",
      "ECE                  0.171768\n",
      "ACE                  0.171675\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1013283/3289410457.py:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(metrics.mean(), ignore_index=True)\n",
      "/tmp/ipykernel_1013283/3289410457.py:45: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  print(metrics.mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06900143801342873\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 342 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m metrics\u001b[38;5;241m.\u001b[39mto_csv(result_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m---> 47\u001b[0m query_set, updated_training_set, updated_poolset \u001b[38;5;241m=\u001b[39m \u001b[43mactive_learning_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mpool_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43minitial_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mpool_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m query_set\u001b[38;5;241m.\u001b[39mto_csv(query_set_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_set_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m initial_set,pool_set\n",
      "File \u001b[0;32m/scratch/work/masooda1/active_learning/utils/utils.py:358\u001b[0m, in \u001b[0;36mactive_learning_loop\u001b[0;34m(trained_model, pool_dataloader, initial_set, pool_set, config, query_set, test_dataloader, test_set)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Get location of TopGuns\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     top_indices \u001b[38;5;241m=\u001b[39m get_top_indices(acquisition, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_query\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 358\u001b[0m     query_set \u001b[38;5;241m=\u001b[39m \u001b[43mget_query_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPIG_MT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m######### EPIG SAMPLING ############\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/work/masooda1/active_learning/utils/data_utils.py:278\u001b[0m, in \u001b[0;36mget_query_set\u001b[0;34m(pool_set, top_indices)\u001b[0m\n\u001b[1;32m    276\u001b[0m p_set \u001b[38;5;241m=\u001b[39m pool_set\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    277\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(p_set, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m mask[top_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    279\u001b[0m p_set[mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# pull our guys (y_ik)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 342 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "for itteration in range(config[\"num_iterations\"]):\n",
    "\n",
    "    from utils.models import Vanilla_MLP_classifier\n",
    "    seed_everything(seed = config[\"seed\"])\n",
    "    config[\"itteration\"] = itteration\n",
    "    config[\"model_name\"] = rf'itteration_{config[\"itteration\"]}_s{config[\"seed\"]}_alpha_{config[\"alpha\"]}_gamma_{config[\"gamma\"]}_loss_type_{config[\"loss_type\"]}_Î»{config[\"optm_l2_lambda\"]}'\n",
    "    config[\"checkpoint_name\"] = f'model-iter{itteration}-{config[\"main_task\"]}-{{epoch:02d}}-{{val_BCE_non_weighted:.4f}}' \n",
    "\n",
    "    # get dataloaders\n",
    "    train_dataloader = convert_dataframe_to_dataloader(dataframe= initial_set, config = config, shuffle= True)\n",
    "    val_dataloader = convert_dataframe_to_dataloader(dataframe= val_set, config = config, shuffle= False)\n",
    "    test_dataloader = convert_dataframe_to_dataloader(dataframe= test_set, config = config, shuffle= False)\n",
    "    pool_dataloader = convert_dataframe_to_dataloader(dataframe= pool_set, config = config, shuffle= False)\n",
    "\n",
    "    # Train model\n",
    "    config[\"training_steps\"] = len(train_dataloader)\n",
    "    trained_model, run, trainer = wandb_init_model(model = Vanilla_MLP_classifier, \n",
    "                                                            train_dataloader = train_dataloader,\n",
    "                                                            val_dataloader =val_dataloader,\n",
    "                                                            config = config, \n",
    "                                                            model_type = 'MLP')\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "    ###### Model Evaluation #############\n",
    "    # make dir\n",
    "    query_set_dir = config[\"metadata_dir\"] + \"query_set/\"\n",
    "    result_dir = config[\"metadata_dir\"]\n",
    "    os.makedirs(query_set_dir, exist_ok = True)\n",
    "    os.makedirs(result_dir, exist_ok = True)\n",
    "\n",
    "    # Evaluation\n",
    "    trained_model = trained_model.eval()\n",
    "    targets, pred_mean, pred_var, all_pred = get_pred_with_uncertainities(\n",
    "                                                                            test_dataloader, trained_model,\n",
    "                                                                            n_classes=config[\"num_of_tasks\"],\n",
    "                                                                            cal_uncert=False,\n",
    "                                                                            num_forward_passes=1\n",
    "                                                                        )\n",
    "    metrics = compute_binary_classification_metrics_MT(targets, pred_mean, missing='nan')\n",
    "\n",
    "    metrics = metrics.append(metrics.mean(), ignore_index=True)\n",
    "    metrics.insert(0, 'Tasks', list(config[\"selected_tasks\"]) + ['mean'])\n",
    "    metrics.to_csv(result_dir + f'metrics_{config[\"model_name\"]}.csv', index=False)\n",
    "    print(metrics.mean())\n",
    "\n",
    "    query_set, updated_training_set, updated_poolset = active_learning_loop(trained_model,\n",
    "                                                                        pool_dataloader, \n",
    "                                                                        initial_set,\n",
    "                                                                        pool_set, \n",
    "                                                                        config)\n",
    "    query_set.to_csv(query_set_dir + f'query_set_{config[\"model_name\"]}.csv', index=False)\n",
    "    del initial_set,pool_set\n",
    "    del train_dataloader,val_dataloader,test_dataloader,pool_dataloader, trained_model\n",
    "\n",
    "    initial_set = updated_training_set.copy()\n",
    "    pool_set = updated_poolset.copy()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    gpu_memory_status = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "    print(\"GPU Memory Status (after clearing):\", gpu_memory_status)\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.insert(0, 'Tasks', list(config[\"selected_tasks\"]) + ['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(y_true, y_prob, n_bins=10, equal_intervals = True):\n",
    "    # Calculate bin boundaries\n",
    "    if equal_intervals == True: # ECE\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    else:                       # ACE\n",
    "        bin_boundaries = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "    \n",
    "    # Calculate bin indices\n",
    "    bin_indices = np.digitize(y_prob, bin_boundaries[1:-1])\n",
    "    \n",
    "    ece = 0\n",
    "    total_samples = len(y_true)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    for bin_idx in range(n_bins):\n",
    "        # Filter samples within the bin\n",
    "        bin_mask = bin_indices == bin_idx\n",
    "        bin_samples = np.sum(bin_mask)\n",
    "        \n",
    "        if bin_samples > 0:\n",
    "            # Calculate accuracy and confidence for the bin\n",
    "            bin_accuracy = np.mean(y_true[bin_mask])\n",
    "            bin_confidence = np.mean(y_prob[bin_mask])\n",
    "        \n",
    "            # Update ECE\n",
    "            ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
    "    \n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17176794644502524"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECE = compute_ece(targets.reshape(-1), pred_mean.reshape(-1), n_bins=10, equal_intervals = True)\n",
    "ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((113,), (113, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape, pred_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_arslan",
   "language": "python",
   "name": "env_arslan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
