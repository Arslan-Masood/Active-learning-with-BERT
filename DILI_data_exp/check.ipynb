{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set [173.0]\n",
      "test_set [49.0]\n",
      "pool_set [123.0]\n",
      "val_set [11.0]\n",
      "initial_set [50.0]\n",
      "train_set [156.0]\n",
      "test_set [49.0]\n",
      "pool_set [106.0]\n",
      "val_set [28.0]\n",
      "initial_set [50.0]\n",
      "train_set [158.0]\n",
      "test_set [49.0]\n",
      "pool_set [108.0]\n",
      "val_set [26.0]\n",
      "initial_set [50.0]\n",
      "train_set [158.0]\n",
      "test_set [49.0]\n",
      "pool_set [108.0]\n",
      "val_set [26.0]\n",
      "initial_set [50.0]\n",
      "train_set [163.0]\n",
      "test_set [49.0]\n",
      "pool_set [113.0]\n",
      "val_set [21.0]\n",
      "initial_set [50.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/projects/home/mmasood1/TG GATE/active_learning/')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import deepchem as dc\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.login(key = \"27edf9c66b032c03f72d30e923276b93aa736429\")\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import gc\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "from utils.data_utils import scafoldsplit_train_test, convert_to_dataframe, get_initial_set_with_main_and_aux_samples, drop_unwanted_tasks\n",
    "from utils.data_utils import convert_dataframe_to_dataloader, convert_df_to_dc_data_object\n",
    "\n",
    "from utils.utils import wandb_init_model, compute_binary_classification_metrics_MT, active_learning_loop\n",
    "from utils.model_utils import get_pred_with_uncertainities\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "seed_list = [1,2,3,4,5]\n",
    "gpu = 0\n",
    "task = \"Y\"\n",
    "for seed in seed_list:\n",
    "    config = {\n",
    "            \"seed\": seed,\n",
    "            # directories\n",
    "            \"project_name\": \"EPIG_Vanilla_BERT_DILI\",\n",
    "            \n",
    "            ### Vanilla EPIG ###\n",
    "            \"metadata_dir\": '/projects/home/mmasood1/trained_model_predictions/DILI/Frozen_BERT/Main_Aux_tasks_setting/1_main_0_aux/',\n",
    "            \"target_dir\": f\"/projects/home/mmasood1/arslan_data_repository/DILI/seed_{seed}/normalized_data/filtered_data/\",        \n",
    "            \"pos_weights\": f\"/projects/home/mmasood1/arslan_data_repository/DILI/seed_{seed}/normalized_data/filtered_data/class_weight_seed_{seed}.csv\",\n",
    "            \n",
    "            # data\n",
    "            \"features_type\" :\"DILI_dataset\",\n",
    "            \"FP_size\" : 1024,\n",
    "            \"train_frac\": 0.8,\n",
    "\n",
    "            # architechture\n",
    "            \"input_dim\": 768,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"depth\" : 1,\n",
    "            \"dropout_p\": 0.3,\n",
    "            \"BatchNorm1d\": True,\n",
    "            \"use_skip_connection\": True,\n",
    "        \n",
    "            # training\n",
    "            \"optim\": 'Adam',#SGD\n",
    "            \"lr_schedulers\": \"CosineAnnealingLR\",\n",
    "            \"lr\": 1e-3,\n",
    "            \"l2_lambda\": 0.0,\n",
    "            \"optm_l2_lambda\": 1e-2,\n",
    "            \"epochs\": 2,\n",
    "            \"compute_metric_after_n_epochs\": 5,\n",
    "            \"batch_size\": 16,\n",
    "            \"EarlyStopping\": False, \n",
    "            \"pretrained_model\": False,\n",
    "            \n",
    "            # loss\n",
    "            \"missing\" : 'nan',\n",
    "            \"alpha\": 0.0,\n",
    "            \"beta\": 0.0,\n",
    "            \"gamma\":0.0,\n",
    "\n",
    "            \"gpu\": [gpu],\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"return_trainer\": True, \n",
    "            \"save_predicitons\" : True,\n",
    "            \"Final_model\": False,\n",
    "\n",
    "            # active learning\n",
    "            \"num_forward_passes\": 20,\n",
    "            \"num_itterations\": 200,\n",
    "            \"sampling_strategy\": \"EPIG_MT\",\n",
    "            \"n_query\":1,\n",
    "            \"main_task_samples\": 100\n",
    "        }\n",
    "    if config[\"gpu\"] == [0]:\n",
    "        config[\"device\"]  = torch.device(\"cuda:0\")\n",
    "    if config[\"gpu\"] == [1]:\n",
    "        config[\"device\"]  = torch.device(\"cuda:1\")\n",
    "    if config[\"gpu\"] == [2]:\n",
    "        config[\"device\"]  = torch.device(\"cuda:2\")\n",
    "    if config[\"gpu\"] == [3]:\n",
    "        config[\"device\"]  = torch.device(\"cuda:3\")\n",
    "    # get targets information\n",
    "\n",
    "    data = pd.read_csv(config[\"target_dir\"] + f\"train_filtered_seed_{seed}.csv\")\n",
    "    all_tasks = list(data.loc[:, \"Y\"].name)\n",
    "    config[\"all_tasks\"] = all_tasks\n",
    "    config[\"main_task\"] = all_tasks\n",
    "    config[\"aux_task\"] = None\n",
    "    config[\"main_task_index\"] = 0\n",
    "    config[\"aux_task_index\"] = None\n",
    "\n",
    "    target_names = config[\"main_task\"]\n",
    "    config[\"project_name\"] = config[\"project_name\"] +\"_\"+ config[\"main_task\"][0]\n",
    "\n",
    "    config[\"num_of_tasks\"] = len(target_names)\n",
    "    config[\"selected_tasks\"] = target_names\n",
    "\n",
    "    config[\"sample_only_from_aux\"] = False\n",
    "    config[\"loss_type\"] = \"BCE\" #\"Focal_loss\",# \"BCE\",\"Focal_loss_v2\"\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    # train, test, val, pool set\n",
    "    train_set = pd.read_csv(config[\"target_dir\"]+ f\"train_filtered_seed_{seed}.csv\")\n",
    "    val_set = pd.read_csv(config[\"target_dir\"]+ f\"valid_filtered_seed_{seed}.csv\")\n",
    "    test_set = pd.read_csv(config[\"target_dir\"]+ f\"test_filtered_seed_{seed}.csv\")\n",
    "\n",
    "    # get dc_data_object\n",
    "    train_set = convert_df_to_dc_data_object(train_set, config)\n",
    "    val_set = convert_df_to_dc_data_object(val_set, config)\n",
    "    test_set = convert_df_to_dc_data_object(test_set, config)\n",
    "\n",
    "    initial_set, pool_set = get_initial_set_with_main_and_aux_samples(train_set, config)\n",
    "\n",
    "    print(\"train_set\", sorted(np.nansum(train_set.y, axis=0)))\n",
    "    print(\"test_set\", sorted(np.nansum(test_set.y, axis=0)))\n",
    "    print(\"pool_set\", sorted(np.nansum(pool_set.y, axis=0)))\n",
    "    print(\"val_set\", sorted(np.nansum(val_set.y, axis=0)))\n",
    "    print(\"initial_set\", sorted(np.nansum(initial_set.y, axis=0)))\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    # Who cares about deepchem data_object, trash it\n",
    "    initial_set = convert_to_dataframe(initial_set, config[\"selected_tasks\"])\n",
    "    val_set = convert_to_dataframe(val_set, config[\"selected_tasks\"])\n",
    "    pool_set = convert_to_dataframe(pool_set, config[\"selected_tasks\"])\n",
    "    test_set = convert_to_dataframe(test_set, config[\"selected_tasks\"])\n",
    "\n",
    "    if config[\"sample_only_from_aux\"]:\n",
    "        pool_set.loc[:,config[\"main_task\"]] = np.nan\n",
    "\n",
    "    # make dir\n",
    "    t_names = config[\"metadata_dir\"] + config[\"sampling_strategy\"] + \"/\" + config[\"main_task\"][0]\n",
    "    query_set_dir = t_names +\"/query_set/\"\n",
    "    result_dir = t_names +\"/Results/\"\n",
    "    config[\"model_weights_dir\"] = t_names +\"/model_weights/\"\n",
    "    os.makedirs(query_set_dir, exist_ok = True)\n",
    "    os.makedirs(result_dir, exist_ok = True)\n",
    "    os.makedirs(config[\"model_weights_dir\"], exist_ok = True)\n",
    "\n",
    "    file_path = query_set_dir + \"initial_set.csv\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        initial_set.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 5\n"
     ]
    }
   ],
   "source": [
    "itteration = 1\n",
    "from utils.models import Vanilla_MLP_classifier\n",
    "seed_everything(seed = config[\"seed\"])\n",
    "config[\"itteration\"] = itteration\n",
    "config[\"model_name\"] = rf'itteration_{config[\"itteration\"]}_s{config[\"seed\"]}_alpha_{config[\"alpha\"]}_gamma_{config[\"gamma\"]}_loss_type_{config[\"loss_type\"]}_Î»{config[\"optm_l2_lambda\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging (100, 768)\n",
      "After merging (47, 768)\n",
      "After merging (94, 768)\n",
      "After merging (221, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /projects/home/mmasood1/trained_model_predictions/DILI/Frozen_BERT/Main_Aux_tasks_setting/1_main_0_aux/EPIG_MT/Y/model_weights/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                   | Type              | Params\n",
      "-------------------------------------------------------------\n",
      "0 | weighted_creterien     | BCEWithLogitsLoss | 0     \n",
      "1 | non_weighted_creterian | BCEWithLogitsLoss | 0     \n",
      "2 | FL                     | FocalLoss         | 0     \n",
      "3 | input_layer            | Linear            | 98.4 K\n",
      "4 | Hidden_block           | ModuleList        | 16.8 K\n",
      "5 | output_layer           | Linear            | 129   \n",
      "6 | dropout                | Dropout           | 0     \n",
      "7 | batchnorm1             | BatchNorm1d       | 256   \n",
      "-------------------------------------------------------------\n",
      "115 K     Trainable params\n",
      "0         Non-trainable params\n",
      "115 K     Total params\n",
      "0.462     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b6597714a74c8583c52edf41ebda05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/mmasood1/.conda/envs/env_arslan/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dc2891343b4ad9a1f39fa50b81cb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38cc139bf5345ac8a8e0b09d21e91c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f958ecbe3904d5ca6d41063269ee30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "train_set_main_active,train_set_main_inactive =  (initial_set[config[\"main_task\"]] == 1).sum().values, (initial_set[config[\"main_task\"]] == 0).sum().values\n",
    "if config[\"aux_task\"] != None:\n",
    "    train_set_aux_active,train_set_aux_inactive = (initial_set[config[\"aux_task\"]] == 1).sum().sum(), (initial_set[config[\"aux_task\"]] == 0).sum().sum()\n",
    "else:\n",
    "    train_set_aux_active,train_set_aux_inactive = np.array([0]),np.array([0])    \n",
    "train_set_main_total = train_set_main_active + train_set_main_inactive\n",
    "train_set_aux_total = train_set_aux_active + train_set_aux_inactive\n",
    "\n",
    "# get dataloaders\n",
    "train_dataloader = convert_dataframe_to_dataloader(dataframe= initial_set, config = config, shuffle= True, drop_last = True)\n",
    "val_dataloader = convert_dataframe_to_dataloader(dataframe= val_set, config = config, shuffle= False, drop_last = False)\n",
    "test_dataloader = convert_dataframe_to_dataloader(dataframe= test_set, config = config, shuffle= False, drop_last = False)\n",
    "pool_dataloader = convert_dataframe_to_dataloader(dataframe= pool_set, config = config, shuffle= False, drop_last = False)\n",
    "\n",
    "# Train model\n",
    "config[\"training_steps\"] = len(train_dataloader)\n",
    "trained_model, run, trainer = wandb_init_model(model = Vanilla_MLP_classifier, \n",
    "                                                        train_dataloader = train_dataloader,\n",
    "                                                        val_dataloader =val_dataloader,\n",
    "                                                        config = config, \n",
    "                                                        model_type = 'MLP')\n",
    "\n",
    "train_set_main_total = train_set_main_active + train_set_main_inactive\n",
    "train_set_aux_total = train_set_aux_active + train_set_aux_inactive\n",
    "wandb.log({\"train_set_main_total\":train_set_main_total.item(),\n",
    "        \"train_set_main_active\":train_set_main_active.item(),\n",
    "        \"train_set_main_inactive\":train_set_main_inactive.item()})\n",
    "\n",
    "wandb.log({\"train_set_aux_total\":train_set_aux_total.item(),\n",
    "        \"train_set_aux_active\":train_set_aux_active.item(),\n",
    "        \"train_set_aux_inactive\":train_set_aux_inactive.item()})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "trained_model = trained_model.eval()\n",
    "targets, pred_mean, pred_var, all_pred = get_pred_with_uncertainities(test_dataloader, trained_model,\n",
    "                                                    n_samples=test_set.shape[0],\n",
    "                                                    n_classes=config[\"num_of_tasks\"],\n",
    "                                                    cal_uncert=True,\n",
    "                                                    num_forward_passes= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29079013],\n",
       "       [0.38216284],\n",
       "       [0.30787875],\n",
       "       [0.6211236 ],\n",
       "       [0.23830371],\n",
       "       [0.22172946],\n",
       "       [0.4160882 ],\n",
       "       [0.73192789],\n",
       "       [0.69951222],\n",
       "       [0.17104351],\n",
       "       [0.28111243],\n",
       "       [0.93334213],\n",
       "       [0.72957969],\n",
       "       [0.71890509],\n",
       "       [0.71121417],\n",
       "       [0.15876704],\n",
       "       [0.29403858],\n",
       "       [0.68864489],\n",
       "       [0.54242337],\n",
       "       [0.66642582],\n",
       "       [0.64567175],\n",
       "       [0.75291109],\n",
       "       [0.78784095],\n",
       "       [0.24423802],\n",
       "       [0.243955  ],\n",
       "       [0.51924188],\n",
       "       [0.75937826],\n",
       "       [0.61018602],\n",
       "       [0.64913057],\n",
       "       [0.11779211],\n",
       "       [0.24692854],\n",
       "       [0.67545281],\n",
       "       [0.39679827],\n",
       "       [0.79698812],\n",
       "       [0.82978136],\n",
       "       [0.17474522],\n",
       "       [0.18181082],\n",
       "       [0.19760751],\n",
       "       [0.5029156 ],\n",
       "       [0.61428587],\n",
       "       [0.55893951],\n",
       "       [0.24114593],\n",
       "       [0.74800767],\n",
       "       [0.38357812],\n",
       "       [0.71021907],\n",
       "       [0.28773498],\n",
       "       [0.55709805],\n",
       "       [0.13417203],\n",
       "       [0.79169029],\n",
       "       [0.20803112],\n",
       "       [0.34245647],\n",
       "       [0.36163394],\n",
       "       [0.34439537],\n",
       "       [0.31121056],\n",
       "       [0.18830173],\n",
       "       [0.91125626],\n",
       "       [0.15944825],\n",
       "       [0.17829717],\n",
       "       [0.30966258],\n",
       "       [0.34570091],\n",
       "       [0.72155087],\n",
       "       [0.53857443],\n",
       "       [0.7588777 ],\n",
       "       [0.74248005],\n",
       "       [0.68206909],\n",
       "       [0.18590797],\n",
       "       [0.21157948],\n",
       "       [0.20048277],\n",
       "       [0.46520967],\n",
       "       [0.34087211],\n",
       "       [0.25308012],\n",
       "       [0.81897727],\n",
       "       [0.63542859],\n",
       "       [0.35950912],\n",
       "       [0.12319483],\n",
       "       [0.13673426],\n",
       "       [0.62701362],\n",
       "       [0.60837611],\n",
       "       [0.27018585],\n",
       "       [0.53564993],\n",
       "       [0.41582288],\n",
       "       [0.59770299],\n",
       "       [0.5219313 ],\n",
       "       [0.18791414],\n",
       "       [0.55724731],\n",
       "       [0.39037026],\n",
       "       [0.86575619],\n",
       "       [0.83689293],\n",
       "       [0.42165923],\n",
       "       [0.29369513],\n",
       "       [0.65865696],\n",
       "       [0.85454037],\n",
       "       [0.92546809],\n",
       "       [0.67147433]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "def enable_dropout(model):\n",
    "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.train()\n",
    "\n",
    "\n",
    "def get_pred_with_uncertainities(dataloader, model, n_samples, n_classes, cal_uncert=False, num_forward_passes=1, device = None):\n",
    "\n",
    "    dropout_predictions = []\n",
    "    for i in range(num_forward_passes):\n",
    "        torch.manual_seed(i)\n",
    "        model = model.eval()\n",
    "        model = model.to(device)\n",
    "        if cal_uncert == True:\n",
    "            enable_dropout(model)\n",
    "        preds, targets = [], []\n",
    "        for batch in dataloader:\n",
    "            batch_x, batch_targets = batch\n",
    "            with torch.no_grad():\n",
    "                batch_preds = model(batch_x.to(device))\n",
    "                batch_preds = batch_preds.cpu().detach().tolist()\n",
    "                preds.extend(batch_preds)\n",
    "                targets.extend(batch_targets.cpu().detach().tolist())\n",
    "                 \n",
    "        preds = expit(np.array(preds))\n",
    "        targets = np.array(targets)\n",
    "        dropout_predictions.append(preds.reshape(-1, n_samples, n_classes))\n",
    "\n",
    "    dropout_predictions = np.concatenate(dropout_predictions, axis=0)\n",
    "        \n",
    "    pred_mean = np.mean(dropout_predictions, axis=0)\n",
    "    pred_var = np.var(dropout_predictions, axis=0)\n",
    "\n",
    "    return targets, pred_mean, pred_var, dropout_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import BALD_acquisition_function, EPIG_MT_acquisition_function, get_top_indices\n",
    "from utils.data_utils import get_random_query_set, get_query_set, update_training_set, remove_queried_index_from_pool_set\n",
    "\n",
    "def active_learning_loop(trained_model,\n",
    "                         pool_dataloader, \n",
    "                         initial_set,\n",
    "                         pool_set, \n",
    "                         config,\n",
    "                         query_set = None,\n",
    "                         test_dataloader = None,\n",
    "                         test_set = None):\n",
    "    if query_set is None:\n",
    "        ####### Uncertainity estmation ################\n",
    "        if config[\"sampling_strategy\"] == \"BALD\":\n",
    "            _, _, _, all_pred = get_pred_with_uncertainities(pool_dataloader, trained_model,\n",
    "                                                            n_samples=pool_set.shape[0],\n",
    "                                                            n_classes=config[\"num_of_tasks\"],\n",
    "                                                            cal_uncert=True,\n",
    "                                                            num_forward_passes=config[\"num_forward_passes\"],\n",
    "                                                            device = config[\"device\"])\n",
    "            acquisition = BALD_acquisition_function(all_pred)\n",
    "            \n",
    "            # We should not query with missing labels, so hide it\n",
    "            nan_mask = ~np.isnan(pool_set[config[\"selected_tasks\"]].values)\n",
    "            acquisition = acquisition * nan_mask\n",
    "\n",
    "            print(np.max(acquisition))\n",
    "\n",
    "            # Get location of TopGuns\n",
    "            top_indices = get_top_indices(acquisition, config[\"n_query\"])\n",
    "            query_set = get_query_set(pool_set, top_indices)\n",
    "\n",
    "        if config[\"sampling_strategy\"] == \"EPIG_MT\":\n",
    "            print(\"######### EPIG SAMPLING ############\")\n",
    "            _, _, _, pool_pred = get_pred_with_uncertainities(pool_dataloader, trained_model,\n",
    "                                                            n_samples=pool_set.shape[0],\n",
    "                                                            n_classes=config[\"num_of_tasks\"],\n",
    "                                                            cal_uncert=True,\n",
    "                                                            num_forward_passes=config[\"num_forward_passes\"],\n",
    "                                                            device = config[\"device\"])\n",
    "\n",
    "            pool_pred = torch.from_numpy(pool_pred)\n",
    "            print(\"step 1\")\n",
    "            _,_, _, test_pred = get_pred_with_uncertainities(test_dataloader, trained_model,\n",
    "                                                            n_samples=test_set.shape[0],\n",
    "                                                            n_classes=config[\"num_of_tasks\"],\n",
    "                                                            cal_uncert=True,\n",
    "                                                            num_forward_passes=config[\"num_forward_passes\"],\n",
    "                                                            device = config[\"device\"])\n",
    "            test_pred = torch.from_numpy(test_pred)\n",
    "            print(\"step 2\")\n",
    "            # we are intereseted in one task\n",
    "            test_pred_main_task = test_pred[:,:,config[\"main_task_index\"]]\n",
    "            test_pred_main_task = torch.unsqueeze(test_pred_main_task, dim = 2)\n",
    "            \n",
    "            acquisition = EPIG_MT_acquisition_function(pool_pred, test_pred_main_task)\n",
    "            acquisition = acquisition.detach().numpy()\n",
    "            \n",
    "            # We should not query with missing labels, so hide it\n",
    "            nan_mask = ~np.isnan(pool_set[config[\"selected_tasks\"]].values)\n",
    "            acquisition = acquisition * nan_mask\n",
    "            print(np.max(acquisition))\n",
    "\n",
    "            # Get location of TopGuns\n",
    "            top_indices = get_top_indices(acquisition, config[\"n_query\"])\n",
    "            query_set = get_query_set(pool_set, top_indices)\n",
    "\n",
    "        if config[\"sampling_strategy\"] == \"uniform\":\n",
    "            query_set = get_random_query_set(pool_set, config)  \n",
    "    else:\n",
    "        print(\"No sample taken\")      \n",
    "    ##########  updated dataset ##################\n",
    "    updated_training_set = update_training_set(initial_set, query_set)\n",
    "    updated_poolset = remove_queried_index_from_pool_set(pool_set, query_set, config)\n",
    "\n",
    "    initial_counts = initial_set.iloc[:, 1:].count().sum()\n",
    "    query_counts = query_set.iloc[:, 1:].count().sum()\n",
    "    updated_training_counts = updated_training_set.iloc[:, 1:].count().sum()\n",
    "\n",
    "    initial_poolset_counts = pool_set.iloc[:, 1:].count().sum()\n",
    "    updated_poolset_counts = updated_poolset.iloc[:, 1:].count().sum()\n",
    "\n",
    "    print(\n",
    "            \"initial_counts\", initial_counts, \n",
    "            \"initial_poolset_count\", initial_poolset_counts, \n",
    "            \"query_counts\", query_counts, \n",
    "            \"updated_training_counts\", updated_training_counts, \n",
    "            \"updated_poolset_counts\", updated_poolset_counts)\n",
    "    \n",
    "    # Use an assertion to check if the counts are equal\n",
    "    assert updated_training_counts == initial_counts + query_counts, \"Training_count,Queryset counts are not equal\"\n",
    "    assert updated_poolset_counts == initial_poolset_counts - query_counts, \"Poolset, Queryset count are not equal\"\n",
    "    return query_set, updated_training_set, updated_poolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### EPIG SAMPLING ############\n",
      "step 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_set, updated_training_set, updated_poolset \u001b[39m=\u001b[39m active_learning_loop(trained_model,\n\u001b[1;32m      2\u001b[0m                                                                                 pool_dataloader, \n\u001b[1;32m      3\u001b[0m                                                                                 initial_set,\n\u001b[1;32m      4\u001b[0m                                                                                 pool_set, \n\u001b[1;32m      5\u001b[0m                                                                                 config)\n",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mactive_learning_loop\u001b[0;34m(trained_model, pool_dataloader, initial_set, pool_set, config, query_set, test_dataloader, test_set)\u001b[0m\n\u001b[1;32m     42\u001b[0m pool_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(pool_pred)\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstep 1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m _,_, _, test_pred \u001b[39m=\u001b[39m get_pred_with_uncertainities(test_dataloader, trained_model,\n\u001b[0;32m---> 45\u001b[0m                                                 n_samples\u001b[39m=\u001b[39mtest_set\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m],\n\u001b[1;32m     46\u001b[0m                                                 n_classes\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mnum_of_tasks\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     47\u001b[0m                                                 cal_uncert\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m                                                 num_forward_passes\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mnum_forward_passes\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     49\u001b[0m                                                 device \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     50\u001b[0m test_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(test_pred)\n\u001b[1;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstep 2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "query_set, updated_training_set, updated_poolset = active_learning_loop(trained_model,\n",
    "                                                                                pool_dataloader, \n",
    "                                                                                initial_set,\n",
    "                                                                                pool_set, \n",
    "                                                                                config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_arslan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
